The decoder and encoder only foundational model code is part of Databricks LLMs: Foundation Models from the Ground Up Course: [LLMs: Foundation Models from the Ground Up](https://learning.edx.org/course/course-v1:Databricks+LLM102x+2T2023/home).

Llama 3 from scratch code is adopted from Naklecha's github repo: [Llama 3 from Scratch](https://github.com/naklecha/llama3-from-scratch/tree/main).

To get more insights in the transformer architectures, one can also look at Harvard's annotated transformer from scratch links: [Link 1](https://nlp.seas.harvard.edu/2018/04/03/attention.html) [Link 2](https://nlp.seas.harvard.edu/annotated-transformer/).
